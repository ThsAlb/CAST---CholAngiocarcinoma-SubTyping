{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-scholarship",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Purpose: To test final model on the test cohorts and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-element",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import all required libraries\n",
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB3\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.utils import resample\n",
    "from numpy import asarray\n",
    "from numpy import savetxt, loadtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-semester",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set current working directory\n",
    "os.chdir(\"/media/data/Projects/LargeDuctVsSmallDuct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooperative-helicopter",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set project directory\n",
    "SSDDir = '/home/thomas/Projects/LargeDuctVsSmallDuct'\n",
    "FiguresDir = SSDDir+'/Figures/InternalTest/'\n",
    "model_dir = SSDDir+'/saved_models/CrossValidation/'\n",
    "TestSetDir = 'Tiles_internal/Sets/Test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-replication",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define positive and negative category\n",
    "PosCategory = 'small'\n",
    "NegCategory = 'large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personalized-necklace",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ungroup the tables for each fold\n",
    "def ungroup_data_table(DataTable):\n",
    "\n",
    "    Tilenames_new = []\n",
    "    Tilenames_flatten = []\n",
    "    Category_new = []\n",
    "    PatientNo_new = []\n",
    "    n = 0\n",
    "\n",
    "    for i in DataTable['Tilenames']:\n",
    "        Tilenames_new.append(i)\n",
    "        for a in range(i.count(', ')+1):\n",
    "            PatientNo_new.append(DataTable.loc[n, 'PatientNo'])\n",
    "            Category_new.append(DataTable.loc[n, 'Category'])\n",
    "        n = n + 1\n",
    "\n",
    "    Tilenames_flatten = [inner for item in Tilenames_new for inner in ast.literal_eval(item)] \n",
    "    Ungrouped_DataTable = pd.DataFrame({'PatientNo': PatientNo_new, 'Category': Category_new, 'Tilenames': Tilenames_flatten, })\n",
    "    return Ungrouped_DataTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-anchor",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ungroup the tables for each fold ver. 2\n",
    "def ungroup_data_table2(DataTable):\n",
    "\n",
    "    Tilenames_new = []\n",
    "    Predictions_new = []\n",
    "    Tilenames_flatten = []\n",
    "    Predictions_flatten = []\n",
    "    Category_new = []\n",
    "    PatientNo_new = []\n",
    "    n = 0\n",
    "\n",
    "    for i in DataTable['Tilenames']:\n",
    "        Tilenames_new.append(i)\n",
    "        Predictions_new.extend(DataTable['Predictions'][n])\n",
    "        for a in range(i.count(', ')+1):\n",
    "            PatientNo_new.append(DataTable.loc[n, 'PatientNo'])\n",
    "            Category_new.append(DataTable.loc[n, 'Category'])\n",
    "        n = n + 1\n",
    "\n",
    "    Tilenames_flatten = [inner for item in Tilenames_new for inner in ast.literal_eval(item)] \n",
    "    Ungrouped_DataTable = pd.DataFrame({'PatientNo': PatientNo_new, 'Category': Category_new, 'Tilenames': Tilenames_flatten, 'Predictions': Predictions_new })\n",
    "    return Ungrouped_DataTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-morrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read TestSet table\n",
    "TestTable = pd.read_csv('/media/data/Projects/LargeDuctVsSmallDuct/Tables/Int_HD_master_TestTable_grouped.csv')\n",
    "TestTableTileLevel = ungroup_data_table(TestTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-religion",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Variable parameters\n",
    "img_height = 300\n",
    "img_width = 300\n",
    "IMAGE_SIZE = [img_height, img_width]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-hierarchy",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define target variable\n",
    "y = TestTable['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-ethics",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create instances of ImageDataGenerator\n",
    "idg_test = ImageDataGenerator(preprocessing_function=preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-listing",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instantiate ImageDataGenerator\n",
    "test_data_generator = idg_test.flow_from_dataframe(TestTableTileLevel, directory = TestSetDir,\n",
    "                                                   x_col = \"Tilenames\", y_col = \"Category\",\n",
    "                                                   batch_size = 64,\n",
    "                                                   target_size = (img_height, img_width),\n",
    "                                                   class_mode = 'binary', shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-mystery",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load final model\n",
    "MyModel = keras.models.load_model(model_dir+'/tuned_model_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-offer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load final model and evaluate\n",
    "results = MyModel.evaluate(test_data_generator)\n",
    "results = dict(zip(MyModel.metrics_names,results))\n",
    "    \n",
    "print('Die Accuracy beträgt ' +str(results['accuracy'])+'.')\n",
    "print('Der Loss beträgt '+str(results['loss'])+'.')\n",
    "        \n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-contest",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate probabilities on test set\n",
    "predictions = MyModel.predict(test_data_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-debut",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read tile and patient thresholds\n",
    "Thresholds=loadtxt('/home/thomas/Projects/LargeDuctVsSmallDuct/Tables/Thresholds_CV.csv', dtype=float, delimiter=',').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-concentrate",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find predicted class and append in list on tile level \n",
    "predicted_class = []\n",
    "for i in predictions:\n",
    "    if i > Thresholds[0]:\n",
    "        predicted_class.append(PosCategory)\n",
    "    else:\n",
    "        predicted_class.append(NegCategory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-celtic",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create tile and patient level prediction tables\n",
    "PredTableTileLevel = TestTableTileLevel.copy()\n",
    "PredTableTileLevel['Predictions'] = predictions\n",
    "PredTableTileLevel['PredictedClass'] = predicted_class\n",
    "\n",
    "PredTablePatientLevel = PredTableTileLevel.groupby(['PatientNo', 'Category'])['Predictions'].agg(list).reset_index()\n",
    "PredTablePatientLevel['Predictions_mean'] = PredTablePatientLevel['Predictions'].apply(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-nursing",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find predicted class and append in list on patient level\n",
    "predictionsmean = PredTablePatientLevel['Predictions_mean']\n",
    "predicted_class2 = []\n",
    "for i in predictionsmean:\n",
    "    if i > Thresholds[1]:\n",
    "        predicted_class2.append(PosCategory)\n",
    "    else:\n",
    "        predicted_class2.append(NegCategory)\n",
    "PredTablePatientLevel['PredictedClass'] = predicted_class2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-burns",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Copy TestTable with predictions\n",
    "TestTable_merg = TestTable.copy()\n",
    "TestTable_merg['Predictions']= PredTablePatientLevel['Predictions']\n",
    "TestTable_merg=TestTable_merg.drop('TileCount', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-enemy",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bootstrapping on patient Level\n",
    "n_iterations = 100\n",
    "stats = list()\n",
    "stats2 = list()\n",
    "accuracy_list1 = []\n",
    "sensitivity_list1 = []\n",
    "specificity_list1 = []\n",
    "ppv_list1 = []\n",
    "npv_list1 = []\n",
    "accuracy_list2 = []\n",
    "sensitivity_list2 = []\n",
    "specificity_list2 = []\n",
    "ppv_list2 = []\n",
    "npv_list2 = []\n",
    "f1score_list1 = []\n",
    "f1score_list2 = []\n",
    "\n",
    "stats_prc = []\n",
    "tprs_prc = []\n",
    "\n",
    "stats2_prc = []\n",
    "tprs2_prc = []\n",
    "\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "tprs = []\n",
    "tprs2 = []\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    print(i)\n",
    "    boot_subset_patient = resample(TestTable_merg)\n",
    "    boot_subset_patient.reset_index(drop=True, inplace=True)\n",
    "    boot_subset = ungroup_data_table2(boot_subset_patient)\n",
    "\n",
    "    predictions_bt = boot_subset['Predictions']\n",
    "    fpr2, tpr2, threshold2 = roc_curve(boot_subset['Category'], predictions_bt, pos_label=PosCategory)\n",
    "    roc_auc = auc(fpr2, tpr2)\n",
    "    stats.append(roc_auc)\n",
    "    interp_tpr = np.interp(mean_fpr, fpr2, tpr2)\n",
    "    interp_tpr[0] = 0.0\n",
    "    tprs.append(interp_tpr)\n",
    "    \n",
    "    precision, recall, thresholds = precision_recall_curve(boot_subset['Category'], predictions_bt, pos_label=PosCategory)\n",
    "    prc_auc = auc(recall, precision)\n",
    "    stats_prc.append(prc_auc)\n",
    "    recall = np.flip(recall)\n",
    "    precision = np.flip(precision)\n",
    "    interp_tpr_prc = np.interp(mean_fpr, recall, precision)\n",
    "    interp_tpr_prc[0] = 1.0\n",
    "    tprs_prc.append(interp_tpr_prc)  \n",
    "        \n",
    "    predicted_class = []\n",
    "    for i in predictions_bt:\n",
    "        if i > Thresholds[0]:\n",
    "            predicted_class.append(PosCategory)\n",
    "        else:\n",
    "            predicted_class.append(NegCategory)\n",
    "    \n",
    "    boot_subset['PredictedClass'] = predicted_class\n",
    "    \n",
    "    large_TileNo = boot_subset.loc[boot_subset['Category'] == 'large'].shape[0]\n",
    "    large_correct = boot_subset.loc[(boot_subset['Category'] == 'large') & (boot_subset['PredictedClass'] == 'large')].shape[0]\n",
    "    large_allPositive = boot_subset.loc[boot_subset['PredictedClass'] == 'large'].shape[0]\n",
    "    small_allPositive = boot_subset.loc[boot_subset['PredictedClass'] == 'small'].shape[0]\n",
    "    large_allNegative = boot_subset.loc[boot_subset['PredictedClass'] == 'small'].shape[0]\n",
    "    large_correctneg = boot_subset.loc[(boot_subset['Category'] == 'large') & (boot_subset['PredictedClass'] == 'large')].shape[0]\n",
    "\n",
    "    small_TileNo = boot_subset.loc[boot_subset['Category'] == 'small'].shape[0]\n",
    "    small_correct = boot_subset.loc[(boot_subset['Category'] == 'small') & (boot_subset['PredictedClass'] == 'small')].shape[0]\n",
    "\n",
    "    accuracy_list1.append(((large_correct+small_correct)/(large_TileNo+small_TileNo))*100)\n",
    "    sensitivity_list1.append((small_correct/small_TileNo)*100)\n",
    "    specificity_list1.append((large_correct/large_TileNo)*100)\n",
    "    ppv_list1.append((small_correct/small_allPositive)*100)\n",
    "    npv_list1.append((large_correctneg/large_allPositive)*100)\n",
    "    f1score_list1.append((f1_score(boot_subset['Category'], boot_subset['PredictedClass'], average = 'macro')))\n",
    "    \n",
    "    TableCopy = boot_subset.copy()\n",
    "    TableCopy = TableCopy.groupby(['PatientNo', 'Category'])['Predictions'].agg(list).reset_index()\n",
    "    TableCopy['Predictions_mean'] = TableCopy['Predictions'].apply(np.mean)\n",
    "    fpr3, tpr3, threshold3 = roc_curve(TableCopy['Category'], TableCopy['Predictions_mean'], pos_label=PosCategory)\n",
    "    roc_auc2 = auc(fpr3, tpr3)\n",
    "    stats2.append(roc_auc2)\n",
    "    interp_tpr2 = np.interp(mean_fpr, fpr3, tpr3)\n",
    "    interp_tpr2[0] = 0.0\n",
    "    tprs2.append(interp_tpr2)\n",
    "    \n",
    "    precision2, recall2, thresholds2 = precision_recall_curve(TableCopy['Category'], TableCopy['Predictions_mean'], pos_label=PosCategory)\n",
    "    prc_auc2 = auc(recall2, precision2)\n",
    "    stats2_prc.append(prc_auc2)\n",
    "    recall2 = np.flip(recall2)\n",
    "    precision2 = np.flip(precision2)\n",
    "    interp_tpr2_prc = np.interp(mean_fpr, recall2, precision2)\n",
    "    interp_tpr2_prc[0] = 1.0\n",
    "    tprs2_prc.append(interp_tpr2_prc)     \n",
    "\n",
    "    predicted_class2 = []\n",
    "    predictionsmean = TableCopy['Predictions_mean']\n",
    "    for i in predictionsmean:\n",
    "        if i > Thresholds[1]:\n",
    "            predicted_class2.append(PosCategory)\n",
    "        else:\n",
    "            predicted_class2.append(NegCategory)\n",
    "    TableCopy['PredictedClass'] = predicted_class2\n",
    "    \n",
    "    large_PatientNo = TableCopy.loc[TableCopy['Category'] == 'large'].shape[0]\n",
    "    large_correct = TableCopy.loc[(TableCopy['Category'] == 'large') & (TableCopy['PredictedClass'] == 'large')].shape[0]\n",
    "    large_allPositive = TableCopy.loc[TableCopy['PredictedClass'] == 'large'].shape[0]\n",
    "    small_allPositive = TableCopy.loc[TableCopy['PredictedClass'] == 'small'].shape[0]\n",
    "    large_allNegative = TableCopy.loc[TableCopy['PredictedClass'] == 'small'].shape[0]\n",
    "    large_correctneg = TableCopy.loc[(TableCopy['Category'] == 'large') & (TableCopy['PredictedClass'] == 'large')].shape[0]\n",
    "\n",
    "    small_PatientNo = TableCopy.loc[TableCopy['Category'] == 'small'].shape[0]\n",
    "    small_correct = TableCopy.loc[(TableCopy['Category'] == 'small') & (TableCopy['PredictedClass'] == 'small')].shape[0]\n",
    "\n",
    "    accuracy_list2.append(((large_correct+small_correct)/(large_PatientNo+small_PatientNo))*100)\n",
    "    sensitivity_list2.append((small_correct/small_PatientNo)*100)\n",
    "    specificity_list2.append((large_correct/large_PatientNo)*100)\n",
    "    ppv_list2.append((small_correct/small_allPositive)*100)\n",
    "    npv_list2.append((large_correctneg/large_allPositive)*100)\n",
    "    f1score_list2.append((f1_score(TableCopy['Category'], TableCopy['PredictedClass'], average = 'macro')))\n",
    "\n",
    "## Calculate confidence interval ROC\n",
    "alpha = 0.95\n",
    "p = ((1.0-alpha)/2.0) * 100\n",
    "lower = max(0.0, np.percentile(stats, p))\n",
    "p = (alpha+((1.0-alpha)/2.0)) * 100\n",
    "upper = min(1.0, np.percentile(stats, p))\n",
    "\n",
    "p = ((1.0-alpha)/2.0) * 100\n",
    "lower2 = max(0.0, np.percentile(stats2, p))\n",
    "p = (alpha+((1.0-alpha)/2.0)) * 100\n",
    "upper2 = min(1.0, np.percentile(stats2, p))\n",
    "\n",
    "## Calculate confidence boundaries ROC\n",
    "alpha = 0.95\n",
    "p = ((1.0-alpha)/2.0) * 100\n",
    "roc_lower = np.percentile(tprs, p, axis=0)\n",
    "p = (alpha+((1.0-alpha)/2.0)) * 100\n",
    "roc_upper = np.percentile(tprs, p, axis=0)\n",
    "\n",
    "alpha = 0.95\n",
    "p = ((1.0-alpha)/2.0) * 100\n",
    "roc_lower2 = np.percentile(tprs2, p, axis=0)\n",
    "p = (alpha+((1.0-alpha)/2.0)) * 100\n",
    "roc_upper2 = np.percentile(tprs2, p, axis=0)\n",
    "\n",
    "\n",
    "## Calculate confidence interval PRC\n",
    "alpha = 0.95\n",
    "p = ((1.0-alpha)/2.0) * 100\n",
    "lower_prc = max(0.0, np.percentile(stats_prc, p))\n",
    "p = (alpha+((1.0-alpha)/2.0)) * 100\n",
    "upper_prc = min(1.0, np.percentile(stats_prc, p))\n",
    "\n",
    "p = ((1.0-alpha)/2.0) * 100\n",
    "lower2_prc = max(0.0, np.percentile(stats2_prc, p))\n",
    "p = (alpha+((1.0-alpha)/2.0)) * 100\n",
    "upper2_prc = min(1.0, np.percentile(stats2_prc, p))\n",
    "\n",
    "## Calculate confidence boundaries PRC\n",
    "alpha = 0.95\n",
    "p = ((1.0-alpha)/2.0) * 100\n",
    "roc_lower_prc = np.percentile(tprs_prc, p, axis=0)\n",
    "p = (alpha+((1.0-alpha)/2.0)) * 100\n",
    "roc_upper_prc = np.percentile(tprs_prc, p, axis=0)\n",
    "\n",
    "alpha = 0.95\n",
    "p = ((1.0-alpha)/2.0) * 100\n",
    "roc_lower2_prc = np.percentile(tprs2_prc, p, axis=0)\n",
    "p = (alpha+((1.0-alpha)/2.0)) * 100\n",
    "roc_upper2_prc = np.percentile(tprs2_prc, p, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charitable-harrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot ROC on tile level\n",
    "fpr2, tpr2, thresholds2 = roc_curve(PredTableTileLevel['Category'], PredTableTileLevel['Predictions'], pos_label=PosCategory)\n",
    "auc_tile = auc(fpr2, tpr2)\n",
    "plt.figure(2)\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth = 1.0, label='No Skill')\n",
    "plt.plot(fpr2, tpr2, linewidth = 1.0, label='AUC = ' + str(format((round(auc_tile,3)),'.3f')), zorder=3)\n",
    "plt.fill_between(mean_fpr, roc_lower, roc_upper, color='moccasin',\n",
    "                 label='95% CI ' +str(format((round((lower),3)),'.3f')) + '-' + str(format((round((upper),3)),'.3f')), zorder=1)\n",
    "plt.xlabel('False positive rate', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('True positive rate', fontsize=12, fontweight='bold')\n",
    "plt.tick_params(axis='both', which='major', labelsize=10)\n",
    "plt.tick_params(axis='both', which='minor', labelsize=10)\n",
    "plt.xlim(-0.02,1.02)\n",
    "plt.ylim(-0.02,1.02)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "leg = plt.legend(loc='lower right', fontsize=8)\n",
    "leg.get_frame().set_linewidth(0.0)\n",
    "plt.gca().spines['left'].set_zorder(2)\n",
    "plt.gca().spines['top'].set_zorder(2)\n",
    "plt.savefig(FiguresDir+'ROC_InternalTest_TileLV.png', dpi=1200, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Die AUC auf Tile-Level beträgt '+str(round(auc_tile,2))+'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-classification",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot ROC on patient level\n",
    "fpr, tpr, thresholds = roc_curve(PredTablePatientLevel['Category'], PredTablePatientLevel['Predictions_mean'], pos_label=PosCategory)\n",
    "auc_patient = auc(fpr, tpr)\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth = 1.0, label='No Skill')\n",
    "plt.plot(fpr, tpr, linewidth = 1.0, label='AUC = ' + str(format((round(auc_patient,3)),'.3f')), zorder=3)\n",
    "plt.fill_between(mean_fpr, roc_lower2, roc_upper2, color='moccasin',\n",
    "                 label='95% CI ' + str(format((round((lower2),3)),'.3f')) + '-' + str(format((round((upper2),3)),'.3f')), zorder=1)\n",
    "plt.xlabel('False positive rate', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('True positive rate', fontsize=12, fontweight='bold')\n",
    "plt.tick_params(axis='both', which='major', labelsize=10)\n",
    "plt.tick_params(axis='both', which='minor', labelsize=10)\n",
    "plt.xlim(-0.02,1.02)\n",
    "plt.ylim(-0.02,1.02)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "leg = plt.legend(loc='lower right', fontsize=8)\n",
    "leg.get_frame().set_linewidth(0.0)\n",
    "plt.gca().spines['left'].set_zorder(2)\n",
    "plt.gca().spines['top'].set_zorder(2)\n",
    "plt.savefig(FiguresDir+'ROC_InternalTest_PatientLV.png', dpi=1200, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Die AUC auf Patient-Level beträgt '+str(round(auc_patient,2))+'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-addition",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot PRC on tile level\n",
    "precision, recall, thresholds = precision_recall_curve(PredTableTileLevel['Category'], PredTableTileLevel['Predictions'], pos_label=PosCategory)\n",
    "auc_patient = auc(recall, precision)\n",
    "plt.figure(2)\n",
    "no_skill = len(PredTableTileLevel[PredTableTileLevel['Category']=='small']) / len(PredTableTileLevel)\n",
    "plt.plot([0,1], [no_skill,no_skill], 'k--', linestyle='--', linewidth=1.0, label='No Skill')\n",
    "plt.plot(recall, precision, linewidth = 1.0, label='AUC = ' + str(format((round(auc_patient,3)),'.3f')),zorder=3)\n",
    "plt.fill_between(mean_fpr, roc_lower_prc, roc_upper_prc, color='moccasin',\n",
    "                 label='95% CI ' + str(format((round((lower_prc),3)),'.3f')) + '-' + str(format((round((upper_prc),3)),'.3f')), zorder=1)\n",
    "plt.xlabel('Recall', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Precision', fontsize=12, fontweight='bold')\n",
    "plt.tick_params(axis='both', which='major', labelsize=10)\n",
    "plt.tick_params(axis='both', which='minor', labelsize=10)\n",
    "plt.xlim(-0.02,1.02)\n",
    "plt.ylim(-0.02,1.02)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "leg = plt.legend(loc='lower right', fontsize=8)\n",
    "leg.get_frame().set_linewidth(0.0)\n",
    "plt.gca().spines['left'].set_zorder(2)\n",
    "plt.gca().spines['top'].set_zorder(2)\n",
    "plt.gca().spines['right'].set_zorder(2)\n",
    "plt.savefig(FiguresDir+'PRC_InternalTest_TileLV.png', dpi=1200, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Die PRC-AUC auf Tile-Level beträgt '+str(round(auc_patient,2))+'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-passport",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot PRC on patient level\n",
    "precision, recall, thresholds = precision_recall_curve(PredTablePatientLevel['Category'], PredTablePatientLevel['Predictions_mean'], pos_label=PosCategory)\n",
    "auc_patient = auc(recall, precision)\n",
    "plt.figure(1)\n",
    "no_skill = len(PredTablePatientLevel[PredTablePatientLevel['Category']=='small']) / len(PredTablePatientLevel)\n",
    "plt.plot([0,1], [no_skill,no_skill], 'k--', linestyle='--', linewidth=1.0, label='No Skill')\n",
    "plt.plot(recall, precision, linewidth = 1.0, label='AUC = ' + str(format((round(auc_patient,3)),'.3f')),zorder=3)\n",
    "plt.fill_between(mean_fpr, roc_lower2_prc, roc_upper2_prc, color='moccasin',\n",
    "                 label='95% CI ' + str(format((round((lower2_prc),3)),'.3f')) + '-' + str(format((round((upper2_prc),3)),'.3f')), zorder=1)\n",
    "plt.xlabel('Recall', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Precision', fontsize=12, fontweight='bold')\n",
    "plt.tick_params(axis='both', which='major', labelsize=10)\n",
    "plt.tick_params(axis='both', which='minor', labelsize=10)\n",
    "plt.xlim(-0.02,1.02)\n",
    "plt.ylim(-0.02,1.02)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "leg = plt.legend(loc='lower right', fontsize=8)\n",
    "leg.get_frame().set_linewidth(0.0)\n",
    "plt.gca().spines['left'].set_zorder(2)\n",
    "plt.gca().spines['top'].set_zorder(2)\n",
    "plt.gca().spines['right'].set_zorder(2)\n",
    "plt.savefig(FiguresDir+'PRC_InternalTest_PatientLV.png', dpi=1200, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Die PRC-AUC auf Patient-Level beträgt '+str(round(auc_patient,2))+'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "congressional-technology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrix on patient level (absolute)\n",
    "skplt.metrics.plot_confusion_matrix(PredTablePatientLevel['Category'], PredTablePatientLevel['PredictedClass'], title = ' ', figsize = (4,3),normalize=False)\n",
    "plt.xlabel('Predicted', fontweight='bold')\n",
    "plt.ylabel('Ground Truth', fontweight='bold')\n",
    "locs, labels = plt.xticks() \n",
    "plt.xticks(locs,['large duct', 'small duct'])\n",
    "locs, labels = plt.yticks() \n",
    "plt.yticks(locs,['large duct', 'small duct'])\n",
    "plt.savefig(FiguresDir+'CoMa_InternalTest_PatientLV_abs.png', dpi=1200, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-cotton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrix on patient level (relative)\n",
    "skplt.metrics.plot_confusion_matrix(PredTablePatientLevel['Category'], PredTablePatientLevel['PredictedClass'], title = ' ', figsize = (4,3),normalize=True)\n",
    "plt.xlabel('Predicted', fontweight='bold')\n",
    "plt.ylabel('Ground Truth', fontweight='bold')\n",
    "locs, labels = plt.xticks() \n",
    "plt.xticks(locs,['large duct', 'small duct'])\n",
    "locs, labels = plt.yticks() \n",
    "plt.yticks(locs,['large duct', 'small duct'])\n",
    "plt.savefig(FiguresDir+'CoMa_InternalTest_PatientLV_rel.png', dpi=1200, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-bikini",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrix on tile level (absolute)\n",
    "skplt.metrics.plot_confusion_matrix(PredTableTileLevel['Category'], PredTableTileLevel['PredictedClass'], title = ' ', figsize = (4,3), normalize=False)\n",
    "plt.xlabel('Predicted', fontweight='bold')\n",
    "plt.ylabel('Ground Truth', fontweight='bold')\n",
    "locs, labels = plt.xticks() \n",
    "plt.xticks(locs,['large duct', 'small duct'])\n",
    "locs, labels = plt.yticks() \n",
    "plt.yticks(locs,['large duct', 'small duct'])\n",
    "plt.savefig(FiguresDir+'CoMa_InternalTest_TileLV_abs.png', dpi=1200, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-basin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrix on tile level (relative)\n",
    "skplt.metrics.plot_confusion_matrix(PredTableTileLevel['Category'], PredTableTileLevel['PredictedClass'], title = ' ', figsize = (4,3), normalize=True)\n",
    "plt.xlabel('Predicted', fontweight='bold')\n",
    "plt.ylabel('Ground Truth', fontweight='bold')\n",
    "locs, labels = plt.xticks() \n",
    "plt.xticks(locs,['large duct', 'small duct'])\n",
    "locs, labels = plt.yticks() \n",
    "plt.yticks(locs,['large duct', 'small duct'])\n",
    "plt.savefig(FiguresDir+'CoMa_InternalTest_TileLV_rel.png', dpi=1200, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "married-surrey",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save prediction tables\n",
    "PredTablePatientLevel.to_csv('/home/thomas/Projects/LargeDuctVsSmallDuct/Tables/PredTablePatientLevel.csv', index=False)\n",
    "PredTableTileLevel.to_csv('/home/thomas/Projects/LargeDuctVsSmallDuct/Tables/PredTableTileLevel.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-peoples",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Compute metrics on tile level. Small duct is disease.\n",
    "Metrics_TileLevel_Test = pd.DataFrame(columns=['Name', 'Accuracy','Sensitivity','Specificity','PPV','NPV', 'F1'])\n",
    "names = ['Original','Lower bound (95%CI)', 'Upper bound (95%CI)']\n",
    "Metrics_TileLevel_Test['Name'] = names\n",
    "accuracy=[]\n",
    "sensitivity=[]\n",
    "specificity=[]\n",
    "ppv =[]\n",
    "npv=[]\n",
    "f1s=[]\n",
    "\n",
    "small_TileNo = PredTableTileLevel.loc[PredTableTileLevel['Category'] == 'small'].shape[0]\n",
    "small_correct = PredTableTileLevel.loc[(PredTableTileLevel['Category'] == 'small') & (PredTableTileLevel['PredictedClass'] == 'small')].shape[0]\n",
    "small_allPositive = PredTableTileLevel.loc[PredTableTileLevel['PredictedClass'] == 'small'].shape[0]\n",
    "small_allNegative = PredTableTileLevel.loc[PredTableTileLevel['PredictedClass'] == 'large'].shape[0]\n",
    "small_correctneg = PredTableTileLevel.loc[(PredTableTileLevel['Category'] == 'large') & (PredTableTileLevel['PredictedClass'] == 'large')].shape[0]\n",
    "\n",
    "large_TileNo = PredTableTileLevel.loc[PredTableTileLevel['Category'] == 'large'].shape[0]\n",
    "large_correct = PredTableTileLevel.loc[(PredTableTileLevel['Category'] == 'large') & (PredTableTileLevel['PredictedClass'] == 'large')].shape[0]\n",
    "\n",
    "accuracy.append(np.round(((small_correct+large_correct)/(small_TileNo+large_TileNo))*100,3))\n",
    "sensitivity.append(np.round((small_correct/small_TileNo)*100,3))\n",
    "specificity.append(np.round((large_correct/large_TileNo)*100,3))\n",
    "ppv.append(np.round((small_correct/small_allPositive)*100,3))\n",
    "npv.append(np.round((small_correctneg/small_allNegative)*100,3))\n",
    "f1s.append(np.round((f1_score(PredTableTileLevel['Category'], PredTableTileLevel['PredictedClass'], average = 'macro')),3))\n",
    "\n",
    "alpha = 0.95\n",
    "p = ((1.0-alpha)/2.0) * 100\n",
    "p1 = (alpha+((1.0-alpha)/2.0)) * 100\n",
    "\n",
    "accuracy.append(np.round(max(0.0, np.percentile(accuracy_list1, p)),3))\n",
    "sensitivity.append(np.round(max(0.0, np.percentile(sensitivity_list1, p)),3))\n",
    "specificity.append(np.round(max(0.0, np.percentile(specificity_list1, p)),3))\n",
    "ppv.append(np.round(max(0.0, np.percentile(ppv_list1, p)),3))\n",
    "npv.append(np.round(max(0.0, np.percentile(npv_list1, p)),3))\n",
    "f1s.append(np.round(max(0.0, np.percentile(f1score_list1, p)),3))        \n",
    "\n",
    "accuracy.append(np.round(min(100.0, np.percentile(accuracy_list1, p1)),3))\n",
    "sensitivity.append(np.round(min(100.0, np.percentile(sensitivity_list1, p1)),3))\n",
    "specificity.append(np.round(min(100.0, np.percentile(specificity_list1, p1)),3))\n",
    "ppv.append(np.round(min(100.0,np.percentile(ppv_list1, p1)),3))\n",
    "npv.append(np.round(min(100.0, np.percentile(npv_list1, p1)),3))\n",
    "f1s.append(np.round(min(100.0, np.percentile(f1score_list1, p1)),3))           \n",
    "\n",
    "Metrics_TileLevel_Test['Accuracy']=accuracy\n",
    "Metrics_TileLevel_Test['Sensitivity']=sensitivity\n",
    "Metrics_TileLevel_Test['Specificity']=specificity\n",
    "Metrics_TileLevel_Test['PPV']=ppv\n",
    "Metrics_TileLevel_Test['NPV']=npv\n",
    "Metrics_TileLevel_Test['F1']=f1s\n",
    "\n",
    "Metrics_TileLevel_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-bobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save tile metrics\n",
    "Metrics_TileLevel_Test.to_csv('/home/thomas/Projects/LargeDuctVsSmallDuct/Tables/Metrics_TileLevel_InternalTest.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cubic-organ",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save lists as Pandas dataframe\n",
    "Bootstrap_internal_TileLevel = pd.DataFrame(columns=['Accuracy', 'Sensitivity','Specificity','PPV','NPV', 'AUC', 'F1'])\n",
    "Bootstrap_internal_TileLevel['Accuracy']=accuracy_list1\n",
    "Bootstrap_internal_TileLevel['Sensitivity']=sensitivity_list1\n",
    "Bootstrap_internal_TileLevel['Specificity']=specificity_list1\n",
    "Bootstrap_internal_TileLevel['PPV']=ppv_list1\n",
    "Bootstrap_internal_TileLevel['NPV']=npv_list1\n",
    "Bootstrap_internal_TileLevel['AUC']=stats\n",
    "Bootstrap_internal_TileLevel['F1']=f1score_list1\n",
    "\n",
    "Bootstrap_internal_TileLevel.to_csv('/home/thomas/Projects/LargeDuctVsSmallDuct/Tables/Bootstrap_internal_TileLevel.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-funds",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Compute metrics on patient level. Small duct is disease.\n",
    "Metrics_PatientLevel_Test = pd.DataFrame(columns=['Name', 'Accuracy','Sensitivity','Specificity','PPV','NPV','F1'])\n",
    "names = ['Original','Lower bound (95%CI)', 'Upper bound (95%CI)']\n",
    "Metrics_PatientLevel_Test['Name'] = names\n",
    "accuracy=[]\n",
    "sensitivity=[]\n",
    "specificity=[]\n",
    "ppv =[]\n",
    "npv=[]\n",
    "f1s=[]\n",
    "\n",
    "small_PatientNo = PredTablePatientLevel.loc[PredTablePatientLevel['Category'] == 'small'].shape[0]\n",
    "small_correct = PredTablePatientLevel.loc[(PredTablePatientLevel['Category'] == 'small') & (PredTablePatientLevel['PredictedClass'] == 'small')].shape[0]\n",
    "small_allPositive = PredTablePatientLevel.loc[PredTablePatientLevel['PredictedClass'] == 'small'].shape[0]\n",
    "small_allNegative = PredTablePatientLevel.loc[PredTablePatientLevel['PredictedClass'] == 'large'].shape[0]\n",
    "small_correctneg = PredTablePatientLevel.loc[(PredTablePatientLevel['Category'] == 'large') & (PredTablePatientLevel['PredictedClass'] == 'large')].shape[0]\n",
    "\n",
    "large_PatientNo = PredTablePatientLevel.loc[PredTablePatientLevel['Category'] == 'large'].shape[0]\n",
    "large_correct = PredTablePatientLevel.loc[(PredTablePatientLevel['Category'] == 'large') & (PredTablePatientLevel['PredictedClass'] == 'large')].shape[0]\n",
    "\n",
    "accuracy.append(np.round(((small_correct+large_correct)/(small_PatientNo+large_PatientNo))*100,3))\n",
    "sensitivity.append(np.round((small_correct/small_PatientNo)*100,3))\n",
    "specificity.append(np.round((large_correct/large_PatientNo)*100,3))\n",
    "ppv.append(np.round((small_correct/small_allPositive)*100,3))\n",
    "npv.append(np.round((small_correctneg/small_allNegative)*100,3))\n",
    "f1s.append(np.round((f1_score(PredTablePatientLevel['Category'], PredTablePatientLevel['PredictedClass'], average = 'macro')),3))\n",
    "\n",
    "alpha = 0.95\n",
    "p = ((1.0-alpha)/2.0) * 100\n",
    "p1 = (alpha+((1.0-alpha)/2.0)) * 100\n",
    "\n",
    "accuracy.append(np.round(max(0.0, np.percentile(accuracy_list2, p)),3))\n",
    "sensitivity.append(np.round(max(0.0, np.percentile(sensitivity_list2, p)),3))\n",
    "specificity.append(np.round(max(0.0, np.percentile(specificity_list2, p)),3))\n",
    "ppv.append(np.round(max(0.0, np.percentile(ppv_list2, p)),3))\n",
    "npv.append(np.round(max(0.0, np.percentile(npv_list2, p)),3))\n",
    "f1s.append(np.round(max(0.0, np.percentile(f1score_list2, p)),3))                 \n",
    "\n",
    "accuracy.append(np.round(min(100.0, np.percentile(accuracy_list2, p1)),3))\n",
    "sensitivity.append(np.round(min(100.0, np.percentile(sensitivity_list2, p1)),3))\n",
    "specificity.append(np.round(min(100.0, np.percentile(specificity_list2, p1)),3))\n",
    "ppv.append(np.round(min(100.0,np.percentile(ppv_list2, p1)),3))\n",
    "npv.append(np.round(min(100.0, np.percentile(npv_list2, p1)),3))\n",
    "f1s.append(np.round(min(100.0, np.percentile(f1score_list2, p1)),3))\n",
    "\n",
    "Metrics_PatientLevel_Test['Accuracy']=accuracy\n",
    "Metrics_PatientLevel_Test['Sensitivity']=sensitivity\n",
    "Metrics_PatientLevel_Test['Specificity']=specificity\n",
    "Metrics_PatientLevel_Test['PPV']=ppv\n",
    "Metrics_PatientLevel_Test['NPV']=npv\n",
    "Metrics_PatientLevel_Test['F1']=f1s\n",
    "\n",
    "\n",
    "Metrics_PatientLevel_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-poster",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save patient metrics\n",
    "Metrics_PatientLevel_Test.to_csv('/home/thomas/Projects/LargeDuctVsSmallDuct/Tables/Metrics_PatientLevel_InternalTest.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-platinum",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save lists as Pandas Dataframe\n",
    "Bootstrap_internal_PatientLevel = pd.DataFrame(columns=['Accuracy', 'Sensitivity','Specificity','PPV','NPV', 'AUC', 'F1'])\n",
    "Bootstrap_internal_PatientLevel['Accuracy']=accuracy_list2\n",
    "Bootstrap_internal_PatientLevel['Sensitivity']=sensitivity_list2\n",
    "Bootstrap_internal_PatientLevel['Specificity']=specificity_list2\n",
    "Bootstrap_internal_PatientLevel['PPV']=ppv_list2\n",
    "Bootstrap_internal_PatientLevel['NPV']=npv_list2\n",
    "Bootstrap_internal_PatientLevel['AUC']=stats2\n",
    "Bootstrap_internal_TileLevel['F1']=f1score_list2\n",
    "\n",
    "Bootstrap_internal_PatientLevel.to_csv('/home/thomas/Projects/LargeDuctVsSmallDuct/Tables/Bootstrap_internal_PatientLevel.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF2.5",
   "language": "python",
   "name": "tf2.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
